import numpy as np
def cross_val(data,k=10):
    """
    k-fold cross validation - splits data into training, test and validation datasets
    :param 
    data: full dataset
    k: number of folds
    """
    test_set = {}
    training_set = {}
    validation_set = {}

    split = len(data)//(k//2)
    validation_split  = len(data)//(k)#splits data into training:testing:validation (2 sweeps  = 10)
    for i in range(k//2): 
        test_set[i] = data[i*split:(i*split)+validation_split]
        validation_set[i] = data[(i*split)+validation_split:(i*split)+split]
        training_set[i] = np.delete(data,slice(i*split,(i*split)+split),axis = 0)
    
    for i in range(k//2):
        index = i+k//2 
        validation_set[index] = data[i*split:(i*split)+validation_split]
        test_set[index] = data[(i*split)+validation_split:(i*split)+split]
        training_set[index] = np.delete(data,slice(i*split,(i*split)+split),axis = 0)

    return test_set,training_set,validation_set

def evaluate(root,test_set,is_pruning=1,confusion_matrix=[[0,0,0,0],[0,0,0,0],[0,0,0,0],[0,0,0,0]]):
    """
    Helper function for calc_avg_metrics and pruning.
    Returns tree performance statistics
    :param 
    root: tree to be evaluated
    test_set: k test sets
    is_pruning: a flag for the prune function
    confusion_matrix: takes a matrix passed by reference to update values. If no matrix given, creates one.
    """
    rooms_actual = {1:0,2:0,3:0,4:0}
    true_positives = {1:0,2:0,3:0,4:0}
    false_positives = {1:0,2:0,3:0,4:0}

    correct = 0
    total = 0

    for row in test_set: #loops through each test case
        total+=1
        rooms_actual[row[-1]]+=1 # for confusion matrix
        prediction = eval_tree(root,row)
        confusion_matrix[(int(row[-1]))-1][(int(prediction))-1] +=1
        if prediction == row[-1]:
            correct+=1
            true_positives[prediction]+=1
        else:
            false_positives[prediction]+=1
       
    accuracy = correct/total
    if is_pruning ==1:
        return accuracy #just returns accuracy if this function is used by pruning
    else:
        return confusion_matrix,accuracy,rooms_actual,true_positives,false_positives          
    

def eval_tree(root, input):
    """ DFS traversal through decision tree and prins leaf nodes

    Input: root node (type: tree)
    return an integer
    """
    
    if not root.node['left'] and not root.node['right']: 
        return root.node['attribute']
    
    attr = root.node['attribute']
    if input[attr] <= root.node['val']:
        return eval_tree(root.node['left'], input)
    else:    
        return eval_tree(root.node['right'], input)   

def get_metrics(rooms_actual,true_positives,false_positives):
    """
    Calculates metrics
    param:
    rooms_actual: count of rooms in test set
    true_positives: true positives generated by evaluate
    false_positives false positives generated by evaluate
    """
    precision = {1:0,2:0,3:0,4:0}
    recall = {1:0,2:0,3:0,4:0}
    f1 ={1:0,2:0,3:0,4:0}
    for room in range(1,5):
        if rooms_actual[room] == 0:
            continue
        f1Denom = precision[room]+recall[room]
        if f1Denom == 0:
            continue
        precision[room] = true_positives[room]/(true_positives[room]+false_positives[room])
        recall[room] = true_positives[room]/rooms_actual[room]
        f1[room] = (2*precision[room]*recall[room])/f1Denom

    return ((precision,recall,f1))  


def calc_avg_metrics(root, test_set, accuracy, precision, recall, f1,confusion_matrix, k=10):
    """
    Averages the metrics
    param:
    root: the tree to be used
    test_set: the test dataset
    accuracy: accuracy generated by evaluate
    precision: precision generated from get_metrics
    recall: recall generated from get_metrics
    f1: f1 generated from get_metrics
    confusion_matrix: confusion matrix passed by reference
    k: number of folds 
    """
    temp_accuracy  = accuracy
    confusion_matrix,accuracy,rooms_actual,true_positives,false_positives = evaluate(root,test_set,0,confusion_matrix)
    accuracy += temp_accuracy

    metrics = get_metrics(rooms_actual,true_positives,false_positives)
    for i in range(1,5):
        precision[i] += metrics[0][i]
        recall[i]+= metrics[1][i]
        f1[i]+= metrics[2][i]
        if i == k-1: #Divides by k
            precision[i]/=k
            recall[i]/=k
            f1[i]/=k
            accuracy/=k

    return confusion_matrix,accuracy,precision,recall,f1

def normalise(precision,recall,f1,conf_matrix, k=10):
    """
    Normalise the metrics by factor of k
    param:
    precision: precision metric
    recall: recall metric
    f1: f1 metric
    conf_matrix: confusion matrix
    k: number of folds 
    """
    for i in precision:
        if precision[i]!=0:
            precision[i]/=k
    for i in recall:
        if recall[i]!=0:
            recall[i]/=k
    for i in f1:
        if f1[i]!=0:
            f1[i]/=k
    for i in range(len(conf_matrix)-1):
        for j in range(len(conf_matrix[i])-1):
            if conf_matrix[i][j]!=0:
                conf_matrix[i][j]/= k
    return precision,recall,f1,conf_matrix